{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7748313,"sourceType":"datasetVersion","datasetId":4529676}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install tensorflow_docs","metadata":{"execution":{"iopub.status.busy":"2024-03-03T15:52:53.639074Z","iopub.execute_input":"2024-03-03T15:52:53.639740Z","iopub.status.idle":"2024-03-03T15:53:05.808769Z","shell.execute_reply.started":"2024-03-03T15:52:53.639709Z","shell.execute_reply":"2024-03-03T15:53:05.807510Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow_docs in /opt/conda/lib/python3.10/site-packages (2023.5.24.56664)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from tensorflow_docs) (1.4.0)\nRequirement already satisfied: astor in /opt/conda/lib/python3.10/site-packages (from tensorflow_docs) (0.8.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from tensorflow_docs) (3.1.2)\nRequirement already satisfied: nbformat in /opt/conda/lib/python3.10/site-packages (from tensorflow_docs) (5.9.2)\nRequirement already satisfied: protobuf>=3.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow_docs) (3.20.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from tensorflow_docs) (6.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->tensorflow_docs) (2.1.3)\nRequirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat->tensorflow_docs) (2.19.1)\nRequirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.10/site-packages (from nbformat->tensorflow_docs) (4.20.0)\nRequirement already satisfied: jupyter-core in /opt/conda/lib/python3.10/site-packages (from nbformat->tensorflow_docs) (5.7.1)\nRequirement already satisfied: traitlets>=5.1 in /opt/conda/lib/python3.10/site-packages (from nbformat->tensorflow_docs) (5.9.0)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->tensorflow_docs) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->tensorflow_docs) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->tensorflow_docs) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->tensorflow_docs) (0.16.2)\nRequirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core->nbformat->tensorflow_docs) (4.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfdata\nimport tensorflow_docs as tfdocs\nimport tensorflow_docs.plots\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.optimizers import RMSprop\nimport os\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n\ndef normalize(input_image, input_mask):\n    input_image = preprocess_input(input_image)\n    input_mask = tf.cast(input_mask, tf.float32) / 255.0  # Normalize mask to the range [0, 1]\n\n    return input_image, input_mask\n\n\n\n@tf.function\ndef load_image(image_file, mask_file, train=True):\n    input_image = tf.io.read_file(image_file)\n    input_image = tf.image.decode_png(input_image, channels=3)\n    input_image = tf.image.resize(input_image, (224, 224))\n\n    input_mask = tf.io.read_file(mask_file)\n    input_mask = tf.image.decode_png(input_mask, channels=1)\n    input_mask = tf.image.resize(input_mask, (224, 224))\n\n    if train and tf.random.uniform(()) > 0.5:\n        input_image = tf.image.flip_left_right(input_image)\n        input_mask = tf.image.flip_left_right(input_mask)\n\n    input_image, input_mask = normalize(input_image, input_mask)\n\n    return input_image, input_mask\n\n\nclass UNet(object):\n    def __init__(self,\n                 input_size=(224, 224, 3),\n                 num_classes=2,\n                 output_channels=1):  # Update output channels based on your mask format\n        self.pretrained_model = MobileNetV2(\n            input_shape=input_size,\n            include_top=False,\n            weights='imagenet')\n\n        self.target_layers = [\n            'block_1_expand_relu',\n            'block_3_expand_relu',\n            'block_6_expand_relu',\n            'block_13_expand_relu',\n            'block_16_project'\n        ]\n\n        self.input_size = input_size\n        self.num_classes = num_classes\n        self.output_channels = output_channels\n        \n\n        self.model = self._create_model()\n        loss = SparseCategoricalCrossentropy(from_logits=True)\n        self.model.compile(optimizer=RMSprop(),\n                           loss=loss,\n                           metrics=['accuracy'])\n\n    @staticmethod\n    def _upsample(filters, size, dropout=False):\n        init = tf.random_normal_initializer(0.0, 0.02)\n\n        layers = Sequential()\n        layers.add(Conv2DTranspose(filters=filters,\n                                   kernel_size=size,\n                                   strides=2,\n                                   padding='same',\n                                   kernel_initializer=init,\n                                   use_bias=False))\n\n        layers.add(BatchNormalization())\n\n        if dropout:\n            layers.add(Dropout(rate=0.5))\n\n        layers.add(ReLU())\n\n        return layers\n\n    def _create_model(self):\n        layers = [self.pretrained_model.get_layer(l).output\n                  for l in self.target_layers]\n        down_stack = Model(inputs=self.pretrained_model.input,\n                           outputs=layers)\n        down_stack.trainable = False\n\n        up_stack = []\n\n        for filters in (512, 256, 128, 64):\n            up_block = self._upsample(filters, 4)\n            up_stack.append(up_block)\n\n        inputs = Input(shape=self.input_size)\n        x = inputs\n\n        skip_layers = down_stack(x)\n        x = skip_layers[-1]\n        skip_layers = reversed(skip_layers[:-1])\n\n        for up, skip_connection in zip(up_stack, skip_layers):\n            x = up(x)\n            x = Concatenate()([x, skip_connection])\n\n        init = tf.random_normal_initializer(0.0, 0.02)\n        output = Conv2DTranspose(\n            filters=self.output_channels,\n            kernel_size=3,\n            strides=2,\n            padding='same',\n            kernel_initializer=init)(x)\n\n        return Model(inputs, outputs=output)\n\n    @staticmethod\n    def _plot_model_history(model_history, metric, ylim=None):\n        plt.style.use('seaborn-darkgrid')\n        plotter = tfdocs.plots.HistoryPlotter()\n        plotter.plot({'Model': model_history}, metric=metric)\n\n        plt.title(f'{metric.upper()}')\n        if ylim is None:\n            plt.ylim([0, 1])\n        else:\n            plt.ylim(ylim)\n\n        plt.savefig(f'{metric}.png')\n        plt.close()\n\n    def train(self, train_dataset, epochs, steps_per_epoch,\n              validation_dataset, validation_steps):\n        hist = \\\n            self.model.fit(train_dataset,\n                           epochs=epochs,\n                           steps_per_epoch=steps_per_epoch,\n                           validation_steps=validation_steps,\n                           validation_data=validation_dataset)\n\n        self._plot_model_history(hist, 'loss', [0., 2.0])\n        self._plot_model_history(hist, 'accuracy')\n\n    @staticmethod\n    def _process_mask(mask):\n        mask = (mask.numpy() * 127.5).astype('uint8')\n        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n\n        return mask\n\n    def _save_image_and_masks(self, image,\n                              ground_truth_mask,\n                              prediction_mask,\n                              image_id):\n        image = (image.numpy() * 255.0).astype('uint8')\n        gt_mask = self._process_mask(ground_truth_mask)\n        pred_mask = self._process_mask(prediction_mask)\n\n        mosaic = np.hstack([image, gt_mask, pred_mask])\n        mosaic = cv2.cvtColor(mosaic, cv2.COLOR_RGB2BGR)\n\n        cv2.imwrite(f'out3_{image_id}.jpg', mosaic)\n\n    @staticmethod\n    def _create_mask(prediction_mask):\n        prediction_mask = tf.argmax(prediction_mask, axis=-1)\n        prediction_mask = prediction_mask[..., tf.newaxis]\n\n        return prediction_mask[0]\n\n    def _save_predictions(self, dataset, sample_size=1):\n        for id, (image, mask) in \\\n                enumerate(dataset.take(sample_size), start=1):\n            pred_mask = self.model.predict(image)\n            pred_mask = self._create_mask(pred_mask)\n\n            image = image[0]\n            ground_truth_mask = mask[0]\n\n            self._save_image_and_masks(image,\n                                       ground_truth_mask,\n                                       pred_mask,\n                                       image_id=id)\n\n    def evaluate(self, test_dataset, sample_size=5):\n        result = self.model.evaluate(test_dataset)\n        print(f'Accuracy: {result[1] * 100:.2f}%')\n\n        self._save_predictions(test_dataset, sample_size)\n\n\n# Path to your custom dataset\nIMAGE_DIR = '/kaggle/input/dataset33/data befor split/images'\nMASK_DIR = '/kaggle/input/dataset33/data befor split/masks'\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n# Load custom dataset and create train and test datasets\nimage_files = sorted([os.path.join(IMAGE_DIR, file) for file in os.listdir(IMAGE_DIR)])\nmask_files = sorted([os.path.join(MASK_DIR, file) for file in os.listdir(MASK_DIR)])\n\n# Split dataset into train and test\nsplit_ratio = 0.8\nsplit_index = int(len(image_files) * split_ratio)\n\ntrain_image_files = image_files[:split_index]\ntrain_mask_files = mask_files[:split_index]\ntest_image_files = image_files[split_index:]\ntest_mask_files = mask_files[split_index:]\nBATCH_SIZE=64\n# Calculate STEPS_PER_EPOCH\nSTEPS_PER_EPOCH = len(train_image_files) // BATCH_SIZE\n# Calculate VALIDATION_STEPS\nVALIDATION_STEPS = len(test_image_files) // BATCH_SIZE\n\n\n# Create train and test datasets\ntrain_dataset = (tf.data.Dataset.from_tensor_slices((train_image_files, train_mask_files))\n                 .map(load_image, num_parallel_calls=AUTOTUNE)\n                 .cache()\n                 .shuffle(len(train_image_files))\n                 .batch(BATCH_SIZE)\n                 .repeat()\n                 .prefetch(buffer_size=AUTOTUNE))\n\ntest_dataset = (tf.data.Dataset.from_tensor_slices((test_image_files, test_mask_files))\n                .map(lambda image, mask: load_image(image, mask, train=False), num_parallel_calls=AUTOTUNE)\n                .batch(BATCH_SIZE))\n\n# Initialize and train UNet model\nunet = UNet()\nunet.train(train_dataset,\n           epochs=10,\n           steps_per_epoch=STEPS_PER_EPOCH,\n           validation_dataset=test_dataset,\n           validation_steps=VALIDATION_STEPS)\n\n# Evaluate the trained model\nunet.evaluate(test_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-03T15:53:05.811726Z","iopub.execute_input":"2024-03-03T15:53:05.812159Z","iopub.status.idle":"2024-03-03T15:56:04.159720Z","shell.execute_reply.started":"2024-03-03T15:53:05.812119Z","shell.execute_reply":"2024-03-03T15:56:04.158689Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-03-03 15:53:06.217746: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-03 15:53:06.217805: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-03 15:53:06.219226: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m 1/42\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28:58\u001b[0m 42s/step - accuracy: 0.8088 - loss: 0.0000e+00","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1709481234.160608     594 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 361ms/step - accuracy: 0.8102 - loss: 0.0000e+00 - val_accuracy: 0.8219 - val_loss: 0.0000e+00\nEpoch 2/10\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 343ms/step - accuracy: 0.8115 - loss: 0.0000e+00 - val_accuracy: 0.8106 - val_loss: 0.0000e+00\nEpoch 3/10\n\u001b[1m 1/42\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 190ms/step - accuracy: 0.8161 - loss: 0.0000e+00","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 286ms/step - accuracy: 0.8108 - loss: 0.0000e+00 - val_accuracy: 0.8171 - val_loss: 0.0000e+00\nEpoch 4/10\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 192ms/step - accuracy: 0.8102 - loss: 0.0000e+00 - val_accuracy: 0.8069 - val_loss: 0.0000e+00\nEpoch 5/10\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 279ms/step - accuracy: 0.8131 - loss: 0.0000e+00 - val_accuracy: 0.8146 - val_loss: 0.0000e+00\nEpoch 6/10\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 192ms/step - accuracy: 0.8111 - loss: 0.0000e+00 - val_accuracy: 0.8051 - val_loss: 0.0000e+00\nEpoch 7/10\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 276ms/step - accuracy: 0.8103 - loss: 0.0000e+00 - val_accuracy: 0.8134 - val_loss: 0.0000e+00\nEpoch 8/10\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 191ms/step - accuracy: 0.8115 - loss: 0.0000e+00 - val_accuracy: 0.8042 - val_loss: 0.0000e+00\nEpoch 9/10\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 277ms/step - accuracy: 0.8099 - loss: 0.0000e+00 - val_accuracy: 0.8128 - val_loss: 0.0000e+00\nEpoch 10/10\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 191ms/step - accuracy: 0.8102 - loss: 0.0000e+00 - val_accuracy: 0.8038 - val_loss: 0.0000e+00\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_552/2090467736.py:133: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn-darkgrid')\n/tmp/ipykernel_552/2090467736.py:133: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn-darkgrid')\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 337ms/step - accuracy: 0.8117 - loss: 0.0000e+00\nAccuracy: 81.22%\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save model weights\nunet.model.save_weights('/kaggle/working/unet_weights.weights.h5')","metadata":{"execution":{"iopub.status.busy":"2024-03-03T15:56:57.968672Z","iopub.execute_input":"2024-03-03T15:56:57.969448Z","iopub.status.idle":"2024-03-03T15:56:58.423377Z","shell.execute_reply.started":"2024-03-03T15:56:57.969414Z","shell.execute_reply":"2024-03-03T15:56:58.422318Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}